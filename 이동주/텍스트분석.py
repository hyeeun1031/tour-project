# -*- coding: utf-8 -*-
"""í…ìŠ¤íŠ¸ë¶„ì„.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u7EoviRyuitzrHukKrR5Bo1Zi2_vhDEY
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install koreanize-matplotlib
# import koreanize_matplotlib

!pip install gensim

import pandas as pd

# íŒŒì¼ ê²½ë¡œì™€ ì¥ì†Œ ì´ë¦„ (ì—…ë¡œë“œëœ ì „í†µë¬¸í™” 3ê°œ íŒŒì¼ ê¸°ì¤€)
files_info = [
    ("/content/bulguksa.csv", "bulguksa"),
    ("/content/andong_village.csv", "andong")
]

# ê° íŒŒì¼ ì²˜ë¦¬
dfs = []
for path, place in files_info:
    try:
        df = pd.read_csv(path, encoding="utf-8", header=None)
    except UnicodeDecodeError:
        df = pd.read_csv(path, encoding="ISO-8859-1", header=None)

    # ì²« ì¤„ì´ í—¤ë”ê±°ë‚˜ BOM ìˆìœ¼ë©´ ì œê±°
    if df.iloc[0, 0].lower().strip() in ['review', 'Ã¯Â»Â¿review']:
        df = df.iloc[1:]

    # ì—¬ëŸ¬ ì—´ì´ë©´ í•œ ì¤„ë¡œ ê²°í•©
    reviews = []
    for row in df.itertuples(index=False):
        line = " ".join(map(str, row)).strip()
        reviews.append(line)

    all_df = pd.DataFrame({"review": reviews})
    all_df["place"] = place
    dfs.append(all_df)

# ë³‘í•©
df_all = pd.concat(dfs, ignore_index=True)

# í™•ì¸
print(df_all.head())

import pandas as pd

# íŒŒì¼ ê²½ë¡œì™€ ì¥ì†Œ ì´ë¦„ (ì—…ë¡œë“œëœ ì „í†µë¬¸í™” 3ê°œ íŒŒì¼ ê¸°ì¤€)
files_info = [
    ("/content/gangwon_mt.csv", "Gangwon"),
    ("/content/namiIsland.csv", "Nami")
]

# ê° íŒŒì¼ ì²˜ë¦¬
dfs = []
for path, place in files_info:
    try:
        df = pd.read_csv(path, encoding="utf-8", header=None)
    except UnicodeDecodeError:
        df = pd.read_csv(path, encoding="ISO-8859-1", header=None)

    # ì²« ì¤„ì´ í—¤ë”ê±°ë‚˜ BOM ìˆìœ¼ë©´ ì œê±°
    if df.iloc[0, 0].lower().strip() in ['review', 'Ã¯Â»Â¿review']:
        df = df.iloc[1:]

    # ì—¬ëŸ¬ ì—´ì´ë©´ í•œ ì¤„ë¡œ ê²°í•©
    reviews = []
    for row in df.itertuples(index=False):
        line = " ".join(map(str, row)).strip()
        reviews.append(line)

    temp_df = pd.DataFrame({"review": reviews})
    temp_df["place"] = place
    dfs.append(temp_df)

# ë³‘í•©
df_traditional = pd.concat(dfs, ignore_index=True)

# í™•ì¸
print(df_traditional.head())

import re
import nltk
from nltk.corpus import stopwords

# nltk ë¦¬ì†ŒìŠ¤ (ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰)
nltk.download('stopwords')
nltk.download('punkt')

# ë¶ˆìš©ì–´ ì„¤ì •
stop_words = set(stopwords.words('english'))

def preprocess_text_simple(text):
    text = text.lower()
    text = re.sub(r"[^a-z\s]", "", text)
    words = text.split()  # ê°„ë‹¨í•œ ê³µë°± ê¸°ì¤€ í† í°í™”
    words = [w for w in words if w not in stop_words and len(w) > 1]
    return " ".join(words)

df_all["cleaned_review"] = df_all["review"].apply(preprocess_text_simple)

from textblob import TextBlob

# ê°ì„± ì ìˆ˜ ë¶€ì—¬ í•¨ìˆ˜
def get_sentiment(text):
    return TextBlob(text).sentiment.polarity  # -1 ~ +1

# ì ìš©
df_all["sentiment_score"] = df_all["cleaned_review"].apply(get_sentiment)

# ê°ì„± ë¼ë²¨ (optional: ê¸/ì¤‘/ë¶€)
def label_sentiment(score):
    if score > 0.1:
        return "positive"
    elif score < -0.1:
        return "negative"
    else:
        return "neutral"

df_all["sentiment_label"] = df_all["sentiment_score"].apply(label_sentiment)

df_all[df_all['sentiment_label']=='positive'].head()

import matplotlib.pyplot as plt

# 1. ê°ì„± ë¼ë²¨ ë¶„í¬ (ë§‰ëŒ€ê·¸ë˜í”„)
label_counts = df_all["sentiment_label"].value_counts()

plt.figure(figsize=(6, 4))
label_counts.plot(kind='bar', color=['green', 'gray', 'red'])
plt.title("Sentiment Label Distribution")
plt.xlabel("Sentiment")
plt.ylabel("Number of Reviews")
plt.xticks(rotation=0)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

# 2. ê°ì„± ì ìˆ˜ ë¶„í¬ (íˆìŠ¤í† ê·¸ë¨)
plt.figure(figsize=(6, 4))
plt.hist(df_all["sentiment_score"], bins=20, color='skyblue', edgecolor='black')
plt.title("Sentiment Score Distribution")
plt.xlabel("Polarity Score (-1 to +1)")
plt.ylabel("Number of Reviews")
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

from textblob import TextBlob

# ê°ì„± ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜
def get_sentiment(text):
    return TextBlob(text).sentiment.polarity  # -1 ~ +1

# ì ìˆ˜ ì ìš©
df_all["sentiment_score"] = df_all["cleaned_review"].apply(get_sentiment)

# ì„¸ë¶€ ê°ì„± ë¼ë²¨ í•¨ìˆ˜
def detailed_sentiment(score):
    if score >= 0.4:
        return "high-positive"
    elif score >= 0.1:
        return "low-positive"
    elif score > -0.1:
        return "neutral"
    else:
        return "negative"

# ê°ì„± êµ¬ê°„ë³„ ë¼ë²¨ ë¶€ì—¬
df_all["sentiment_detail"] = df_all["sentiment_score"].apply(detailed_sentiment)

from wordcloud import WordCloud
import matplotlib.pyplot as plt


# 1. ê¸ì •(high-positive) ë¦¬ë·° í…ìŠ¤íŠ¸ ê²°í•©
positive_text = " ".join(df_all[df_all["sentiment_detail"] == "high-positive"]["cleaned_review"])

# 2. ë¶€ì •(negative) ë¦¬ë·° í…ìŠ¤íŠ¸ ê²°í•©
negative_text = " ".join(df_all[df_all["sentiment_detail"] == "negative"]["cleaned_review"])

# 3. ê¸ì • ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
wordcloud_pos = WordCloud(
    width=800,
    height=400,
    background_color='white',
    colormap='Reds'  # ë¶‰ì€ ê³„ì—´
).generate(positive_text)

# 4. ë¶€ì • ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
wordcloud_neg = WordCloud(
    width=800,
    height=400,
    background_color='white',
    colormap='Greys'  # íšŒìƒ‰ ê³„ì—´
).generate(negative_text)

# 5. ì‹œê°í™”: ê¸ì •
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_pos, interpolation='bilinear')
plt.axis("off")
plt.title("ê¸ì • ë¦¬ë·° ì›Œë“œí´ë¼ìš°ë“œ", fontsize=16)
plt.show()

# 6. ì‹œê°í™”: ë¶€ì •
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_neg, interpolation='bilinear')
plt.axis("off")
plt.title("ë¶€ì • ë¦¬ë·° ì›Œë“œí´ë¼ìš°ë“œ", fontsize=16)
plt.show()

# ë¦¬ë·° ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ ì„œ source í• ë‹¹ (ì˜ˆ: bulguksa: 0~N, hahoe: N~)
import seaborn as sns

N = 250  # ë¶ˆêµ­ì‚¬ ë¦¬ë·° ìˆ˜ (ì •í™•í•œ ìˆ˜ë¡œ êµì²´í•´ì•¼ í•¨)

df_all.loc[:N-1, "source"] = "Bulguksa"
df_all.loc[N:, "source"] = "Hahoe"

plt.figure(figsize=(8, 4))
sns.countplot(data=df_all, x="sentiment_label", hue="source", palette="pastel")
plt.title("ë¶ˆêµ­ì‚¬ vs í•˜íšŒë§ˆì„ ê°ì„± ë¼ë²¨ ë¶„í¬")
plt.xlabel("Sentiment")
plt.ylabel("Review Count")
plt.legend(title="Place")
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import networkx as nx
from collections import Counter
from itertools import combinations
from gensim.utils import simple_preprocess

# í† í°í™”
df_all["tokens"] = df_all["cleaned_review"].apply(lambda x: simple_preprocess(x))


# 1. ê¸ì •(high-positive) ë¦¬ë·°ë§Œ í•„í„°ë§
tokens_pos = df_all[df_all["sentiment_detail"] == "high-positive"]["tokens"]

# 2. ê³µí†µ ë“±ì¥ ë‹¨ì–´ìŒ ì¶”ì¶œ
cooccurrence_pos = []
for tokens in tokens_pos:
    tokens = list(set(tokens))  # ì¤‘ë³µ ì œê±°
    cooccurrence_pos += list(combinations(tokens, 2))

# 3. ë‹¨ì–´ìŒ ë¹ˆë„ ê³„ì‚°
pair_counts_pos = Counter(cooccurrence_pos)
top_pairs_pos = pair_counts_pos.most_common(100)  # ìƒìœ„ 100ê°œ ë‹¨ì–´ìŒ

# 4. ë„¤íŠ¸ì›Œí¬ ìƒì„±
G_pos = nx.Graph()
for (w1, w2), count in top_pairs_pos:
    G_pos.add_edge(w1, w2, weight=count)

# 5. ì‹œê°í™”
plt.figure(figsize=(12, 10))
pos = nx.spring_layout(G_pos, k=0.5, seed=42)  # ë°°ì¹˜ ê³ ì •
weights = [d['weight'] for (u, v, d) in G_pos.edges(data=True)]

nx.draw(
    G_pos, pos,
    with_labels=True,
    node_size=700,
    font_size=10,
    edge_color=weights,
    edge_cmap=plt.cm.Reds,
    width=2,
    edge_vmin=min(weights),
    edge_vmax=max(weights)

)

plt.title("positive ë¦¬ë·° ê¸°ë°˜ ì˜ë¯¸ì—°ê²°ë§")
plt.show()

#negative ë¦¬ë·°ë§Œ í•„í„°ë§
tokens_high = df_all[df_all["sentiment_detail"] == "negative"]["tokens"]

#ê³µí†µ ë“±ì¥ ë‹¨ì–´ìŒ ì¶”ì¶œ
cooccurrence_high = []
for tokens in tokens_high:
    tokens = list(set(tokens))  # ì¤‘ë³µ ì œê±°
    cooccurrence_high += list(combinations(tokens, 2))

#ë‹¨ì–´ìŒ ë¹ˆë„ ê³„ì‚°
pair_counts_high = Counter(cooccurrence_high)
top_pairs_high = pair_counts_high.most_common(100)

#ë„¤íŠ¸ì›Œí¬ ìƒì„±
G_high = nx.Graph()
for (w1, w2), count in top_pairs_high:
    G_high.add_edge(w1, w2, weight=count)

#ì‹œê°í™”
plt.figure(figsize=(12, 10))
pos = nx.spring_layout(G_high, k=0.5, seed=42)
weights = [d['weight'] for (u, v, d) in G_high.edges(data=True)]

nx.draw(G_high, pos, with_labels=True, node_size=700, font_size=10,
        edge_color=weights, edge_cmap=plt.cm.Greens, width=2,
        edge_vmin=min(weights), edge_vmax=max(weights))
plt.title("negative ë¦¬ë·° ê¸°ë°˜ ì˜ë¯¸ì—°ê²°ë§")
plt.show()

# 1. ë¶€ì • ë¦¬ë·°ë§Œ í•„í„°ë§
df_negative = df_all[df_all["sentiment_detail"] == "negative"]

# 2. ì£¼ìš” ì»¬ëŸ¼ë§Œ ì„ íƒí•´ ë³´ê¸° ì¢‹ê²Œ ì •ë¦¬
df_negative_subset = df_negative[['review', 'cleaned_review', 'sentiment_score']]

# 3. ìƒìœ„ 5ê°œ ë¯¸ë¦¬ ë³´ê¸°
print(df_negative_subset.head())

# (ì„ íƒ) 4. CSVë¡œ ì €ì¥í•˜ê³  ì‹¶ì„ ê²½ìš°
# df_negative_subset.to_csv("negative_reviews_only.csv", index=False)

df_negative[df_negative['source'] == 'Bulguksa']  # ë¶ˆêµ­ì‚¬ ë¶€ì • ë¦¬ë·°ë§Œ

df_negative[df_negative['source'] == 'Hahoe']     # í•˜íšŒë§ˆì„ ë¶€ì • ë¦¬ë·°ë§Œ

from collections import Counter

# ê¸ì • ë¦¬ë·° í•µì‹¬ ë‹¨ì–´
positive_tokens = df_all[df_all["sentiment_detail"] == "high-positive"]["tokens"]
positive_words = [word for tokens in positive_tokens for word in tokens]
positive_counter = Counter(positive_words)
top_positive_words = positive_counter.most_common(20)

# ë¶€ì • ë¦¬ë·° í•µì‹¬ ë‹¨ì–´
negative_tokens = df_all[df_all["sentiment_detail"] == "negative"]["tokens"]
negative_words = [word for tokens in negative_tokens for word in tokens]
negative_counter = Counter(negative_words)
top_negative_words = negative_counter.most_common(20)

# ê²°ê³¼ ì¶œë ¥
print("âœ… ê¸ì • ë¦¬ë·° í•µì‹¬ ë‹¨ì–´ Top 20")
for word, count in top_positive_words:
    print(f"{word}: {count}")

print("\nâŒ ë¶€ì • ë¦¬ë·° í•µì‹¬ ë‹¨ì–´ Top 20")
for word, count in top_negative_words:
    print(f"{word}: {count}")

import networkx as nx
from collections import Counter
from itertools import combinations

# 1. ê¸ì •(high-positive) ì˜ë¯¸ì—°ê²°ë§ êµ¬ì„±
tokens_pos = df_all[df_all["sentiment_detail"] == "high-positive"]["tokens"]
cooccurrence_pos = []
for tokens in tokens_pos:
    cooccurrence_pos += list(combinations(set(tokens), 2))  # ì¤‘ë³µ ì œê±°ëœ ë‹¨ì–´ìŒ

pair_counts_pos = Counter(cooccurrence_pos)
top_pairs_pos = pair_counts_pos.most_common(100)

G_pos_top = nx.Graph()
for (w1, w2), count in top_pairs_pos:
    G_pos_top.add_edge(w1, w2, weight=count)

# 2. ë¶€ì •(negative) ì˜ë¯¸ì—°ê²°ë§ êµ¬ì„±
tokens_neg = df_all[df_all["sentiment_detail"] == "negative"]["tokens"]
cooccurrence_neg = []
for tokens in tokens_neg:
    cooccurrence_neg += list(combinations(set(tokens), 2))

pair_counts_neg = Counter(cooccurrence_neg)
top_pairs_neg = pair_counts_neg.most_common(100)

G_neg_top = nx.Graph()
for (w1, w2), count in top_pairs_neg:
    G_neg_top.add_edge(w1, w2, weight=count)

# 3. ì¤‘ì‹¬ì„± ê³„ì‚° (Degree Centrality ê¸°ì¤€)
pos_degree_centrality = nx.degree_centrality(G_pos_top)
top_pos_nodes = sorted(pos_degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]

neg_degree_centrality = nx.degree_centrality(G_neg_top)
top_neg_nodes = sorted(neg_degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]

# 4. ê²°ê³¼ ì¶œë ¥
print("âœ… ê¸ì • ë¦¬ë·° í•µì‹¬ ë‹¨ì–´ Top 10 (degree ì¤‘ì‹¬ì„± ê¸°ì¤€)")
for word, score in top_pos_nodes:
    print(f"{word}: {score:.3f}")

print("\nâŒ ë¶€ì • ë¦¬ë·° í•µì‹¬ ë‹¨ì–´ Top 10 (degree ì¤‘ì‹¬ì„± ê¸°ì¤€)")
for word, score in top_neg_nodes:
    print(f"{word}: {score:.3f}")

traditional_paths = [
    "/content/namiIsland.csv",
    "/content/gangwon_mt.csv"
]

traditional_dfs = []
for path in traditional_paths:
    df = pd.read_csv(path, encoding="ISO-8859-1", header=None)

    # BOM ë¬¸ì œë¡œ ì²« ì¤„ì´ ì»¬ëŸ¼ëª…ì¸ ê²½ìš° ì œê±°
    if df.iloc[0, 0].lower().strip() in ['review', 'Ã¯Â»Â¿review']:
        df = df.iloc[1:]  # ì²« ì¤„ ì œê±°

    df.columns = ["review"]
    traditional_dfs.append(df)

# ë³‘í•©
df_traditional = pd.concat(traditional_dfs, ignore_index=True)

# ìƒìœ„ 5ê°œ í™•ì¸
print(df_traditional.head())

import re
from textblob import TextBlob

# ê°„ë‹¨í•œ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ ì •ì˜ (ì§ì ‘ ì‘ì„±í•œ ìµœì†Œ ë²„ì „)
basic_stopwords = {
    "the", "and", "is", "in", "it", "of", "to", "a", "i", "this", "was", "for", "on",
    "with", "we", "they", "at", "an", "as", "are", "you", "be", "not", "that", "but"
}

# ì „ì²˜ë¦¬ í•¨ìˆ˜ (NLTK ì—†ì´)
def preprocess_text_simple(text):
    if pd.isna(text):
        return ""
    text = text.lower()
    text = re.sub(r"[^a-z\s]", "", text)
    words = text.split()
    words = [w for w in words if w not in basic_stopwords and len(w) > 1]
    return " ".join(words)

# ì „ì²˜ë¦¬ ë° í† í°í™”
df_traditional["cleaned_review"] = df_traditional["review"].apply(preprocess_text_simple)
df_traditional["tokens"] = df_traditional["cleaned_review"].apply(lambda x: x.split())

# ê°ì„± ì ìˆ˜ ê³„ì‚°
df_traditional["sentiment_score"] = df_traditional["cleaned_review"].apply(lambda x: TextBlob(x).sentiment.polarity)

# ê°ì„± ë¼ë²¨ë§
def detailed_sentiment(score):
    if score >= 0.4:
        return "high-positive"
    elif score >= 0.1:
        return "low-positive"
    elif score > -0.1:
        return "neutral"
    else:
        return "negative"

df_traditional["sentiment_detail"] = df_traditional["sentiment_score"].apply(detailed_sentiment)

# ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
df_traditional[["review", "cleaned_review", "sentiment_score", "sentiment_detail"]].head()

import pandas as pd
import re



# í•œê¸€ í¬í•¨ ì—¬ë¶€ë¥¼ íŒë³„í•˜ëŠ” í•¨ìˆ˜
def contains_korean(text):
    return bool(re.search(r"[ê°€-í£]", str(text)))

# í•œêµ­ì–´ê°€ í¬í•¨ëœ í–‰ ì œê±°
df_cleaned = df_traditional[~df_traditional['review'].apply(contains_korean)].reset_index(drop=True)

# ê²°ê³¼ ì €ì¥ (í•„ìš” ì‹œ íŒŒì¼ë¡œ ì €ì¥ ê°€ëŠ¥)
df_cleaned.to_csv("/content/namiIsland_no_korean.csv", index=False, encoding="utf-8-sig")
print("âœ… í•œêµ­ì–´ê°€ í¬í•¨ëœ ë¦¬ë·° ì œê±° ì™„ë£Œ: /content/namiIsland_no_korean.csv")

import pandas as pd

# ìœ ì ì§€ì™€ ì „í†µë¬¸í™” ë¦¬ë·° í•©ì¹˜ê¸°
combined_df = pd.concat([df_all, df_traditional], ignore_index=True)

# ê²°ê³¼ í™•ì¸
print(combined_df["source"].value_counts())  # ì¶œì²˜ë³„ ê°œìˆ˜ í™•ì¸
combined_df.head()

import pandas as pd
import matplotlib.pyplot as plt
import networkx as nx
from gensim.utils import simple_preprocess
from collections import Counter
from itertools import combinations

# 1. ê°„ë‹¨í•œ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ ì •ì˜
custom_stopwords = set([
    "the", "and", "is", "was", "it", "of", "to", "a", "in", "that", "for", "on",
    "with", "this", "as", "at", "an", "be", "but", "we", "they", "i", "our", "my",
    "you", "had", "were", "are", "have", "from", "so", "very", "if", "or", "not",
    "there", "when", "what", "which", "has", "also", "just", "out", "more", "their"
])

# 2. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜
def tokenize(text):
    if pd.isna(text):
        return []
    tokens = simple_preprocess(text)
    return [word for word in tokens if word not in custom_stopwords and len(word) > 1]

# 3. ê¸ì • ë¦¬ë·°ë§Œ ì¶”ì¶œ
pos_df = combined_df[combined_df["sentiment_label"] == "positive"].copy()
pos_df["tokens"] = pos_df["cleaned_review"].apply(tokenize)

# 4. ì „í†µë¬¸í™” vs ìœ ì ì§€ ë¶„ë¦¬
tokens_heritage = pos_df[pos_df["source"].isin(["Bulguksa", "Hahoe"])]["tokens"]
tokens_traditional = pos_df[pos_df["source"].str.startswith("K-Traditional")]["tokens"]

# 5. ì˜ë¯¸ì—°ê²°ë§ ìƒì„± í•¨ìˆ˜
def build_cooccurrence_graph(token_lists, top_n=50):
    pairs = []
    for tokens in token_lists:
        tokens = list(set(tokens))
        pairs.extend(combinations(tokens, 2))
    pair_counts = Counter(pairs).most_common(top_n)

    G = nx.Graph()
    for (w1, w2), count in pair_counts:
        G.add_edge(w1, w2, weight=count)
    return G

# 6. ê·¸ë˜í”„ ìƒì„±
G_pos_heritage = build_cooccurrence_graph(tokens_heritage)
G_pos_traditional = build_cooccurrence_graph(tokens_traditional)

# 7. ì‹œê°í™” í•¨ìˆ˜
def draw_network(G, title, cmap):
    if len(G.edges) == 0:
        print(f"[{title}] ì˜ë¯¸ì—°ê²°ë§ì— ìœ íš¨í•œ ë‹¨ì–´ìŒì´ ì—†ì–´ ì‹œê°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    plt.figure(figsize=(10, 8))
    pos = nx.spring_layout(G, k=0.5, seed=42)
    weights = [d['weight'] for (u, v, d) in G.edges(data=True)]
    nx.draw(G, pos, with_labels=True, node_size=700, font_size=10,
            edge_color=weights, edge_cmap=cmap, width=2,
            edge_vmin=min(weights), edge_vmax=max(weights))
    plt.title(title)
    plt.show()

# 8. ì‹œê°í™”
draw_network(G_pos_heritage, "ìœ ì ì§€ ê¸ì • ë¦¬ë·° ì˜ë¯¸ì—°ê²°ë§", plt.cm.Blues)
draw_network(G_pos_traditional, "ì „í†µë¬¸í™” ê¸ì • ë¦¬ë·° ì˜ë¯¸ì—°ê²°ë§", plt.cm.Reds)

import networkx as nx
import matplotlib.pyplot as plt
from collections import Counter
from itertools import combinations

# 1. ë¶€ì • ë¦¬ë·°ë§Œ í•„í„°ë§
neg_df = combined_df[combined_df["sentiment_label"] == "negative"].copy()

# 2. í† í°í™” í•¨ìˆ˜ (ì´ì „ì— ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•¨)
def tokenize(text):
    return [word for word in text.lower().split() if word.isalpha() and len(word) > 1]

neg_df["tokens"] = neg_df["cleaned_review"].apply(tokenize)

# 3. ì „í†µë¬¸í™” vs ìœ ì ì§€ë¡œ ë‚˜ëˆ„ê¸°
tokens_heritage_neg = neg_df[neg_df["source"].isin(["Bulguksa", "Hahoe"])]["tokens"]
tokens_traditional_neg = neg_df[neg_df["source"].str.startswith("K-Traditional")]["tokens"]

# 4. ì˜ë¯¸ì—°ê²°ë§ ìƒì„± í•¨ìˆ˜
def build_cooccurrence_graph(token_lists, top_n=50):
    pair_counter = Counter()
    for tokens in token_lists:
        unique_tokens = list(set(tokens))  # ì¤‘ë³µ ì œê±°
        pairs = combinations(unique_tokens, 2)
        pair_counter.update(pairs)
    top_pairs = pair_counter.most_common(top_n)

    G = nx.Graph()
    for (w1, w2), count in top_pairs:
        G.add_edge(w1, w2, weight=count)
    return G

# 5. ì‹œê°í™” í•¨ìˆ˜
def draw_network(G, title, cmap):
    if len(G.edges) == 0:
        print(f"[{title}] ì—°ê²°ëœ ë‹¨ì–´ìŒì´ ì—†ì–´ ì‹œê°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    pos = nx.spring_layout(G, k=0.5, seed=42)
    weights = [d['weight'] for (u, v, d) in G.edges(data=True)]

    plt.figure(figsize=(12, 10))
    nx.draw(
        G, pos, with_labels=True, node_size=700, font_size=10,
        edge_color=weights, edge_cmap=cmap, width=2,
        edge_vmin=min(weights), edge_vmax=max(weights)
    )
    plt.title(title)
    plt.show()

# 6. ì˜ë¯¸ì—°ê²°ë§ ìƒì„± ë° ì‹œê°í™”
G_neg_heritage = build_cooccurrence_graph(tokens_heritage_neg, top_n=50)
G_neg_traditional = build_cooccurrence_graph(tokens_traditional_neg, top_n=50)

draw_network(G_neg_heritage, "ìœ ì ì§€ ë¶€ì • ë¦¬ë·° ì˜ë¯¸ì—°ê²°ë§", plt.cm.Reds)
draw_network(G_neg_traditional, "ì „í†µë¬¸í™” ë¶€ì • ë¦¬ë·° ì˜ë¯¸ì—°ê²°ë§", plt.cm.Purples)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# 1. ê¸ì • ë¦¬ë·° í•„í„°ë§ ë° ì—°ê²°
pos_reviews = combined_df[combined_df["sentiment_label"] == "positive"]
text_pos = " ".join(pos_reviews["cleaned_review"].dropna())

# 2. ë¶€ì • ë¦¬ë·° í•„í„°ë§ ë° ì—°ê²°
neg_reviews = combined_df[combined_df["sentiment_label"] == "negative"]
text_neg = " ".join(neg_reviews["cleaned_review"].dropna())

# 3. ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
wc_pos = WordCloud(
    width=800, height=400, background_color='white', colormap='Greens'
).generate(text_pos)

wc_neg = WordCloud(
    width=800, height=400, background_color='white', colormap='Reds'
).generate(text_neg)

# 4. ì‹œê°í™”
plt.figure(figsize=(16, 8))

# ê¸ì • ì›Œë“œí´ë¼ìš°ë“œ
plt.subplot(1, 2, 1)
plt.imshow(wc_pos, interpolation='bilinear')
plt.axis("off")
plt.title("ê¸ì • ë¦¬ë·° ì›Œë“œí´ë¼ìš°ë“œ")

# ë¶€ì • ì›Œë“œí´ë¼ìš°ë“œ
plt.subplot(1, 2, 2)
plt.imshow(wc_neg, interpolation='bilinear')
plt.axis("off")
plt.title("ë¶€ì • ë¦¬ë·° ì›Œë“œí´ë¼ìš°ë“œ")

plt.tight_layout()
plt.show()

"""1. ê¸ì • ì˜ë¯¸ì—°ê²°ë§ & ì›Œë“œí´ë¼ìš°ë“œ í•´ì„

âœ… **ì „ë°˜ì  ê²½í–¥**
- ê¸ì • ê°ì„±ì˜ ë¦¬ë·°ì—ì„œëŠ” beautiful, peaceful, experience, traditional, must, view, nature, calm, history ë“±ì˜ ë‹¨ì–´ë“¤ì´ ì„œë¡œ ê°•í•˜ê²Œ ì—°ê²°ë˜ì–´ ìˆì—ˆìŒ.

- ì›Œë“œí´ë¼ìš°ë“œì—ì„œë„ beautiful, place, visit, recommend, amazing, enjoy ë“±ì˜ ë‹¨ì–´ê°€ í¬ê²Œ ë‚˜íƒ€ë‚¬ê³ , ì´ëŠ” ì¥ì†Œì˜ ì•„ë¦„ë‹¤ì›€ê³¼ ì¶”ì²œ ì˜í–¥ì´ ë§ì´ ì–¸ê¸‰ë˜ì—ˆìŒì„ ëœ»í•´.

- ğŸ¯ ìœ ì ì§€ ë¦¬ë·°(ë¶ˆêµ­ì‚¬/í•˜íšŒë§ˆì„) ì¤‘ì‹¬ ë‹¨ì–´
temple, architecture, history, heritage, walk, calm
â†’ ê³ ìš”í•œ ë¶„ìœ„ê¸°, ì „í†µì  ê±´ì¶•ë¬¼, ì—­ì‚¬ì„±ì´ ê°•ì ìœ¼ë¡œ ì¸ì‹ë¨.

- ğŸ ì „í†µë¬¸í™” ë¦¬ë·°(í•œë³µÂ·ì²´í—˜ ì¤‘ì‹¬) ì¤‘ì‹¬ ë‹¨ì–´
hanbok, experience, wear, photo, try, traditional, fun, beautiful
â†’ ì²´í—˜ ìì²´ì˜ ì¦ê±°ì›€ê³¼ ì‚¬ì§„ ì°ëŠ” í™œë™ì´ ê¸ì •ì ìœ¼ë¡œ í‰ê°€ë¨.

ğŸ” 2. ë¶€ì • ì˜ë¯¸ì—°ê²°ë§ & ì›Œë“œí´ë¼ìš°ë“œ í•´ì„

â— **ì „ë°˜ì  ê²½í–¥**
- ë¶€ì • ê°ì„±ì—ì„œëŠ” crowded, hot, long, wait, tired, small, expect, not worth, boring ë“± ë¶ˆí¸í•¨ê³¼ ê¸°ëŒ€ ë¯¸ë‹¬ ê´€ë ¨ ë‹¨ì–´ë“¤ì´ ì—°ê²°ë˜ì–´ ìˆì—ˆìŒ.

- ì›Œë“œí´ë¼ìš°ë“œ ì—­ì‹œ crowded, disappointed, nothing, hard, waste ë“±ì˜ ë‹¨ì–´ê°€ ë¶€ê°ë¨.

- ğŸ¯ ìœ ì ì§€ ë¦¬ë·° ë¶€ì • í‚¤ì›Œë“œ
crowded, tourist, hot, hard, walk, boring
â†’ ì—¬ë¦„ì²  ë°©ë¬¸ì˜ í”¼ë¡œê°, í˜¼ì¡í•¨, ê´€ê´‘ì§€ ìƒì—…í™”ì— ëŒ€í•œ í”¼ë¡œê°€ ë“œëŸ¬ë‚¨.

- ğŸ ì „í†µë¬¸í™” ë¦¬ë·° ë¶€ì • í‚¤ì›Œë“œ
ë¦¬ë·° ìˆ˜ëŠ” ì ì—ˆì§€ë§Œ expected, not much, too short, touristy ë“±
â†’ ì²´í—˜ì˜ ê¹Šì´ ë¶€ì¡±ì´ë‚˜ ê´€ê´‘ ìƒí’ˆí™”ëœ ì²´í—˜ì— ëŒ€í•œ ì•„ì‰¬ì›€ì´ ë‚˜íƒ€ë‚¨.
"""

